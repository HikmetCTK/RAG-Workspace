# Multi Modal Hybrid RAG
![hybrid_multi_v3](https://github.com/user-attachments/assets/80c26821-085a-4554-bb70-53df62f7f516)

## Overviewüîé

This project demonstrates an advanced Retrieval-Augmented Generation (RAG) system capable of answering questions based on information extracted from multiple sources within a document: standard text, structured tables, and images. It leverages a hybrid retrieval approach combining keyword search (BM25) and semantic search (FAISS with Google Embeddings) to find the most relevant context, which is then fed to Google's Gemini model to generate accurate, grounded answers. The system includes specific handling for table data extracted using **Azure Document Analysis** and image descriptions generated by Gemini Vision. A user-friendly interface is provided using Gradio.

## Features‚öôÔ∏è

*   **Multi-Modal Data Handling:** Processes text from PDFs, structured table data (from JSON), and image content.
*   **Table Extraction:** Utilizes pre-processed table data extracted via Azure Document Analysis (provided as a JSON file).
*   **Image Understanding:** Uses Gemini Vision (`gemini-2.0-flash`) to generate descriptions of images within the document context.
*   **PDF Parsing:** Loads and splits PDF document text into manageable chunks using LangChain.
*   **Hybrid Retrieval:** Employs an `EnsembleRetriever` combining:
    *   **BM25:** Efficient keyword-based retrieval for text and table data.
    *   **FAISS Vector Store:** Semantic similarity search using Google's `embedding-001` model for text and image descriptions.
*   **Generative Answering:** Uses Google's `gemini-2.0-flash` model with a specific system prompt to generate answers grounded in the retrieved context, including citations (page numbers or table identifiers).
*   **Gradio Interface:** Provides a simple web UI for users to input queries and receive answers.

## WorkflowüîÄ

1.  **Load Environment Variables:** Loads the `GEMINI_API_KEY` from a `.env` file.
2.  **Table Data Processing:**
    *   Loads table structure data from a JSON file (generated by Azure Document Analysis).
    *   `extract_infos`: Parses the JSON to extract individual cell content, row/column indices, and page numbers.
    *   `flatten_table`: Groups cells by table ID and formats them into descriptive strings per row.
    *   `table_document`: Converts each table's flattened data into a LangChain `Document` object with appropriate metadata (table ID, page label).
3.  **Image Data Processing:**
    *   `get_image_description`: Sends a specified image file to the Gemini Vision model to get a textual description.
    *   Converts the description into a LangChain `Document` with page number metadata.
4.  **PDF Text Processing:**
    *   `parse_split_pdf`: Loads the specified PDF file using `PyPDFLoader`.
    *   Splits the document text into overlapping chunks using `RecursiveCharacterTextSplitter`.
5.  **Retriever Setup:**
    *   `ensemble_retriever`: Initializes:
        *   A `BM25Retriever` using the PDF text chunks and table documents.
        *   A `FAISS` vector store and retriever using the PDF text chunks and image description documents, embedded with `GoogleGenerativeAIEmbeddings`.
        *   An `EnsembleRetriever` combining BM25 and FAISS retrievers with equal weighting.
6.  **Question Answering:**
    *   `get_answer`:
        *   Takes a user query.
        *   Uses the `hybrid_retriever` to fetch relevant documents (text chunks, table data, image descriptions).
        *   Constructs a context string from the retrieved documents, including metadata (page/table info).
        *   Creates a system prompt instructing Gemini to answer based *only* on the provided context and to cite sources.
        *   Calls the Gemini model to generate the final answer.
7.  **Web Interface:**
    *   `main`: Orchestrates the data loading, processing, and retriever setup.
    *   Initializes and launches a Gradio interface for user interaction.
