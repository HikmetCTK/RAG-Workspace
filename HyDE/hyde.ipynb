{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "api_key=os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI,GoogleGenerativeAIEmbeddings\n",
    "generation_config = {\n",
    "            \"temperature\": 0.2,\n",
    "            \"top_p\": 0.9,\n",
    "            \"max_output_tokens\": 4096,\n",
    "            \"response_mime_type\": \"application/json\"\n",
    "        }\n",
    "llm=ChatGoogleGenerativeAI(model=\"gemini-pro\",generation_config=generation_config)\n",
    "embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "load=WebBaseLoader(\"https://www.falkordb.com/blog/advanced-rag/\")\n",
    "data=load.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "docs=text.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vector_stores=FAISS.from_documents(documents=docs,embedding=embedding)\n",
    "retriever=vector_stores.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":3})\n",
    "vector_stores.save_local(\"smote_index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human:  \\nGiven the question: What is Self-Query Retrieval in rag ?, write a detailed and informative passage that provides an answer, explanation, or context about the topic. Be specific and concise, \\nfocusing on relevant facts, examples, and insights. The response should resemble an excerpt from an article, book, or scholarly discussion.\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate #HYDE LOGİC \n",
    "template=\"\"\" \n",
    "Given the question: {question}, write a detailed and informative passage that provides an answer, explanation, or context about the topic. Be specific and concise, \n",
    "focusing on relevant facts, examples, and insights. The response should resemble an excerpt from an article, book, or scholarly discussion.\n",
    "\"\"\"\n",
    "prompt_hyde=ChatPromptTemplate.from_template(template)\n",
    "user_question=\"What is Self-Query Retrieval in rag ?\"\n",
    "query=prompt_hyde.format(question=user_question)  #format provide replacing your variable into the variable in template. 'question' is variable in template 'user question' is your variable out of template . it has been replaced\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Self-Query Retrieval in RAG (Self-Query Retrieval with Retrieval-Augmented Generation) is a novel retrieval-based approach to question answering introduced by Lewis et al. (2020). Unlike traditional question answering systems that rely solely on document retrieval or generative language models, Self-Query Retrieval combines both approaches to achieve superior performance.\\n\\nThe key idea behind Self-Query Retrieval is to use a pretrained retrieval model to identify relevant documents for a given question. These documents are then used to generate a self-query, which is a reformulation of the original question that is more specific and informative. The self-query is then used to retrieve a new set of documents, which are used to generate the final answer.\\n\\nSelf-Query Retrieval has several advantages over traditional question answering systems. First, it allows the system to leverage the strengths of both retrieval and generative models. Second, it enables the system to handle complex questions that require multiple steps of reasoning. Third, it improves the system's ability to generate accurate and informative answers.\\n\\nSelf-Query Retrieval has been shown to achieve state-of-the-art results on a variety of question answering datasets. In particular, it has been shown to outperform traditional question answering systems on questions that require multiple steps of reasoning or that contain complex language.\\n\\nSelf-Query Retrieval is a promising new approach to question answering that has the potential to significantly improve the performance of question answering systems. It is likely to be used in a variety of applications, including search engines, chatbots, and virtual assistants.\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothetical_answer=llm.invoke(query).content\n",
    "hypothetical_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.falkordb.com/blog/advanced-rag/', 'title': 'Advanced RAG Techniques: What They Are & How to Use Them', 'description': 'Master advanced RAG techniques to enhance AI performance, accuracy, and efficiency. Learn methods for optimizing retrieval and generation in complex queries.', 'language': 'en-US'}, page_content='Self-RAG is an advanced technique that empowers your system to refine its own retrieval and generation process by iterating on its outputs. In Self-RAG, the model doesn’t just rely on the initial retrieval but actively re-evaluates and adjusts its approach by generating follow-up queries and responses. This iterative process allows the model to correct its own mistakes, fill in gaps, and enhance the quality of the final output.You can think of Self-RAG as your model’s ability to self-correct'),\n",
       " Document(metadata={'source': 'https://www.falkordb.com/blog/advanced-rag/', 'title': 'Advanced RAG Techniques: What They Are & How to Use Them', 'description': 'Master advanced RAG techniques to enhance AI performance, accuracy, and efficiency. Learn methods for optimizing retrieval and generation in complex queries.', 'language': 'en-US'}, page_content='Self-RAG as your model’s ability to self-correct and improve its answers. By generating an initial response, evaluating its accuracy and relevance, and then adjusting the retrieval process accordingly, the model can produce more nuanced and accurate answers. This approach is particularly useful in scenarios where a single round of retrieval might not be sufficient to provide the best possible answer. With Self-RAG, you enable your system to be more adaptive and precise.'),\n",
       " Document(metadata={'source': 'https://www.falkordb.com/blog/advanced-rag/', 'title': 'Advanced RAG Techniques: What They Are & How to Use Them', 'description': 'Master advanced RAG techniques to enhance AI performance, accuracy, and efficiency. Learn methods for optimizing retrieval and generation in complex queries.', 'language': 'en-US'}, page_content='Retrieval-Augmented Generation (RAG) has become a mainstream approach for working with large language models (LLMs) since its introduction in early research. At its core, RAG gathers knowledge from various sources and generates answers using a language model. However, with basic RAG, also known as Naive RAG, you may encounter challenges in obtaining accurate results for complex queries and face slow response times and higher costs when dealing with large datasets.To address these challenges,')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get relevant documents from document based on output from llm \n",
    "relevant_docs=retriever.invoke(hypothetical_answer)\n",
    "relevant_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Self-RAG is an advanced technique that empowers your system to refine its own retrieval and generation process by iterating on its outputs. In Self-RAG, the model doesn’t just rely on the initial retrieval but actively re-evaluates and adjusts its approach by generating follow-up queries and responses. This iterative process allows the model to correct its own mistakes, fill in gaps, and enhance the quality of the final output.You can think of Self-RAG as your model’s ability to self-correct/nSelf-RAG as your model’s ability to self-correct and improve its answers. By generating an initial response, evaluating its accuracy and relevance, and then adjusting the retrieval process accordingly, the model can produce more nuanced and accurate answers. This approach is particularly useful in scenarios where a single round of retrieval might not be sufficient to provide the best possible answer. With Self-RAG, you enable your system to be more adaptive and precise./nRetrieval-Augmented Generation (RAG) has become a mainstream approach for working with large language models (LLMs) since its introduction in early research. At its core, RAG gathers knowledge from various sources and generates answers using a language model. However, with basic RAG, also known as Naive RAG, you may encounter challenges in obtaining accurate results for complex queries and face slow response times and higher costs when dealing with large datasets.To address these challenges,']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_texts=[\"/n\".join(doc.page_content for doc in relevant_docs)]\n",
    "relevant_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "template=\"\"\"  \n",
    "answer the following question in detailed based on context:\n",
    "context:{context}\n",
    "question:{question}\n",
    "\"\"\"\n",
    "prompt=ChatPromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=prompt.format(context=relevant_texts,question=user_question) ##this response is taken by hyde technique\n",
    "response=llm.invoke(query)\n",
    "answer=response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Query Retrieval (Self-RAG) is an advanced technique in Retrieval-Augmented Generation (RAG) that enables a system to refine its own retrieval and generation process iteratively. Unlike basic RAG, where the model relies solely on the initial retrieval, Self-RAG involves the model actively re-evaluating and adjusting its approach by generating follow-up queries and responses. This iterative process allows the model to correct its own mistakes, fill in gaps, and enhance the quality of the final output.\n",
      "\n",
      "In Self-RAG, the model generates an initial response, evaluates its accuracy and relevance, and then adjusts the retrieval process accordingly. This approach is particularly useful in scenarios where a single round of retrieval might not be sufficient to provide the best possible answer. By enabling the model to self-correct and improve its answers, Self-RAG contributes to more nuanced and accurate responses.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this response is taken  by basic rag implementation\n",
    "relevant_docs2=retriever.invoke(user_question)\n",
    "relevant_texts2=[\"/n\".join(doc.page_content for doc in relevant_docs2)]\n",
    "query2=template.format(context=relevant_texts2,question=user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=llm.invoke(query2)\n",
    "answer=response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Query Retrieval (Self-RAG) is an advanced technique used in Retrieval-Augmented Generation (RAG) systems. It allows the model to iteratively refine its retrieval and generation process by generating follow-up queries and responses.\n",
      "\n",
      "In Self-RAG, the model doesn't rely solely on the initial retrieval but actively re-evaluates and adjusts its approach. This iterative process enables the model to:\n",
      "\n",
      "1. Correct its own mistakes by identifying and addressing inconsistencies or errors in the initial retrieval.\n",
      "\n",
      "2. Fill in gaps by retrieving additional relevant documents or knowledge snippets that were not captured in the initial retrieval.\n",
      "\n",
      "3. Enhance the quality of the final output by synthesizing the retrieved information more effectively and generating more nuanced and accurate responses.\n",
      "\n",
      "Self-RAG empowers the system to be more adaptive and precise by enabling it to self-correct and improve its answers. This is particularly useful in scenarios where a single round of retrieval might not be sufficient to provide the best possible answer.\n",
      "\n",
      "Overall, Self-RAG is a powerful technique that enhances the capabilities of RAG systems by allowing them to refine their own retrieval and generation process, leading to more accurate and comprehensive responses.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from dotenv import load_dotenv\\nimport os\\nload_dotenv()\\napi_key=os.getenv(\"GEMINI_API_KEY\")\\nfrom langchain_google_genai import GoogleGenerativeAI,ChatGoogleGenerativeAI,GoogleGenerativeAIEmbeddings\\nllm=ChatGoogleGenerativeAI(model=\"gemini-pro\",convert_system_message_to_human=True,api_key=api_key)\\nembedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",)\\nfrom langchain_community.document_loaders import PyPDFLoader\\n\\nload=PyPDFLoader(\"2201.08528v3.pdf\")\\ndata=load.load()\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\ntext=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)\\ndocs=text.split_documents(data)\\nfrom langchain_community.vectorstores import FAISS\\nvector_stores=FAISS.from_documents(documents=docs,embedding=embedding)\\nretriever=vector_stores.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":3})'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "api_key=os.getenv(\"GEMINI_API_KEY\")\n",
    "from langchain_google_genai import GoogleGenerativeAI,ChatGoogleGenerativeAI,GoogleGenerativeAIEmbeddings\n",
    "llm=ChatGoogleGenerativeAI(model=\"gemini-pro\",convert_system_message_to_human=True,api_key=api_key)\n",
    "embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",)\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "load=PyPDFLoader(\"2201.08528v3.pdf\")\n",
    "data=load.load()\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "docs=text.split_documents(data)\n",
    "from langchain_community.vectorstores import FAISS\n",
    "vector_stores=FAISS.from_documents(documents=docs,embedding=embedding)\n",
    "retriever=vector_stores.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":3})\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
