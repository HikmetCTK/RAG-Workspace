{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "api_key=os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI,GoogleGenerativeAIEmbeddings\n",
    "generation_config = {\n",
    "            \"temperature\": 0.2,\n",
    "            \"top_p\": 0.9,\n",
    "            \"max_output_tokens\": 4096,\n",
    "            \"response_mime_type\": \"application/json\"\n",
    "        }\n",
    "llm=ChatGoogleGenerativeAI(model=\"gemini-pro\",generation_config=generation_config)\n",
    "embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader  #load data\n",
    "\n",
    "load=WebBaseLoader(\"https://www.falkordb.com/blog/advanced-rag/\")\n",
    "data=load.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter  #text_splitter\n",
    "text=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "docs=text.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS   #vector_Store \n",
    "vector_stores=FAISS.from_documents(documents=docs,embedding=embedding)\n",
    "retriever=vector_stores.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":3})  #similarity search and maximum no chunk is 3\n",
    "vector_stores.save_local(\"rag_index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human:  \\nGiven the question: What is Self-Query Retrieval in rag ?, write a detailed and informative passage that provides an answer, explanation, or context about the topic. Be specific and concise, \\nfocusing on relevant facts, examples, and insights. The response should resemble an excerpt from an article, book, or scholarly discussion.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate #HyDE logic\n",
    "template=\"\"\" \n",
    "Given the question: {question}, write a detailed and informative passage that provides an answer, explanation, or context about the topic. Be specific and concise, \n",
    "focusing on relevant facts, examples, and insights. The response should resemble an excerpt from an article, book, or scholarly discussion.\n",
    "\"\"\"\n",
    "prompt_hyde=ChatPromptTemplate.from_template(template)\n",
    "user_question=\"What is Self-Query Retrieval in rag ?\"\n",
    "query=prompt_hyde.format(question=user_question)  #format provides replacing your variable into the variable in template. 'question' is variable in template 'user question' is your variable out of template . it has been replaced\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Self-Query Retrieval in RAG**\\n\\nSelf-Query Retrieval (SQR) is a technique used in Retrieval-Augmented Generation (RAG) models, where a query is generated from the input context and used to retrieve relevant passages from a knowledge base. This enhances the model\\'s ability to generate informative and coherent responses.\\n\\nIn RAG models, the input context is first encoded into a vector representation. A query generator module then creates a query based on the encoded context. This query is used to search for relevant passages in a pre-built knowledge base. The retrieved passages are then used to augment the input context and provide additional information for response generation.\\n\\nSQR offers several advantages:\\n\\n* **Relevance:** By generating a query from the input context, SQR ensures that retrieved passages are highly relevant to the user\\'s question.\\n* **Efficiency:** SQR reduces the search space by focusing on passages that are likely to contain useful information, making the retrieval process more efficient.\\n* **Enhanced Generation:** The retrieved passages provide additional context and knowledge that the model can leverage to generate more informative and coherent responses.\\n\\nFor example, consider a RAG model trained on a news article corpus. Given the input context \"The United States has imposed sanctions on Russia,\" SQR could generate the query \"What are the reasons for US sanctions on Russia?\" The retrieved passages would provide information on the specific reasons for the sanctions, which the model could use to generate a response such as \"The US has imposed sanctions on Russia due to its annexation of Crimea and its support for separatists in eastern Ukraine.\"\\n\\nOverall, SQR plays a crucial role in RAG models by ensuring the relevance and efficiency of knowledge retrieval, ultimately leading to improved response generation capabilities.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothetical_answer=llm.invoke(query).content \n",
    "hypothetical_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.falkordb.com/blog/advanced-rag/', 'title': 'Advanced RAG Techniques: What They Are & How to Use Them', 'description': 'Master advanced RAG techniques to enhance AI performance, accuracy, and efficiency. Learn methods for optimizing retrieval and generation in complex queries.', 'language': 'en-US'}, page_content='Every RAG application can be broken down into two phases: retrieval and generation. First, RAG retrieves relevant documents or knowledge snippets from external sources, such as knowledge graphs or vector stores, using search and indexing techniques. This retrieved data is then fed into a language model, which generates contextually rich and accurate responses by synthesizing the retrieved information with its pre-trained knowledge.RAG systems have evolved as the requirements have become more'),\n",
       " Document(metadata={'source': 'https://www.falkordb.com/blog/advanced-rag/', 'title': 'Advanced RAG Techniques: What They Are & How to Use Them', 'description': 'Master advanced RAG techniques to enhance AI performance, accuracy, and efficiency. Learn methods for optimizing retrieval and generation in complex queries.', 'language': 'en-US'}, page_content='Self-RAG is an advanced technique that empowers your system to refine its own retrieval and generation process by iterating on its outputs. In Self-RAG, the model doesn’t just rely on the initial retrieval but actively re-evaluates and adjusts its approach by generating follow-up queries and responses. This iterative process allows the model to correct its own mistakes, fill in gaps, and enhance the quality of the final output.You can think of Self-RAG as your model’s ability to self-correct'),\n",
       " Document(metadata={'source': 'https://www.falkordb.com/blog/advanced-rag/', 'title': 'Advanced RAG Techniques: What They Are & How to Use Them', 'description': 'Master advanced RAG techniques to enhance AI performance, accuracy, and efficiency. Learn methods for optimizing retrieval and generation in complex queries.', 'language': 'en-US'}, page_content='Retrieval-Augmented Generation (RAG) has become a mainstream approach for working with large language models (LLMs) since its introduction in early research. At its core, RAG gathers knowledge from various sources and generates answers using a language model. However, with basic RAG, also known as Naive RAG, you may encounter challenges in obtaining accurate results for complex queries and face slow response times and higher costs when dealing with large datasets.To address these challenges,')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#According to the answer to our question answered by llm, getting the relevant chunks from the document \n",
    "relevant_docs=retriever.invoke(hypothetical_answer)\n",
    "relevant_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Every RAG application can be broken down into two phases: retrieval and generation. First, RAG retrieves relevant documents or knowledge snippets from external sources, such as knowledge graphs or vector stores, using search and indexing techniques. This retrieved data is then fed into a language model, which generates contextually rich and accurate responses by synthesizing the retrieved information with its pre-trained knowledge.RAG systems have evolved as the requirements have become more/nSelf-RAG is an advanced technique that empowers your system to refine its own retrieval and generation process by iterating on its outputs. In Self-RAG, the model doesn’t just rely on the initial retrieval but actively re-evaluates and adjusts its approach by generating follow-up queries and responses. This iterative process allows the model to correct its own mistakes, fill in gaps, and enhance the quality of the final output.You can think of Self-RAG as your model’s ability to self-correct/nRetrieval-Augmented Generation (RAG) has become a mainstream approach for working with large language models (LLMs) since its introduction in early research. At its core, RAG gathers knowledge from various sources and generates answers using a language model. However, with basic RAG, also known as Naive RAG, you may encounter challenges in obtaining accurate results for complex queries and face slow response times and higher costs when dealing with large datasets.To address these challenges,']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_texts=[\"/n\".join(doc.page_content for doc in relevant_docs)]\n",
    "relevant_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general template for rag implementations\n",
    "template=\"\"\"  \n",
    "answer the following question in detailed based on context:\n",
    "context:{context}\n",
    "question:{question}\n",
    "\"\"\"\n",
    "prompt=ChatPromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=prompt.format(context=relevant_texts,question=user_question) #^^# this response is taken based on HyDE Method \n",
    "response=llm.invoke(query)\n",
    "hyde_answer=response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Query Retrieval is an advanced technique that empowers your RAG system to refine its own retrieval and generation process by iterating on its outputs. In Self-Query Retrieval, the model doesn’t just rely on the initial retrieval but actively re-evaluates and adjusts its approach by generating follow-up queries and responses. This iterative process allows the model to correct its own mistakes, fill in gaps, and enhance the quality of the final output. You can think of Self-Query Retrieval as your model’s ability to self-correct.\n"
     ]
    }
   ],
   "source": [
    "print(hyde_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this response is taken  from relevant documents based on user question\n",
    "relevant_docs2=retriever.invoke(user_question)\n",
    "relevant_texts2=[\"/n\".join(doc.page_content for doc in relevant_docs2)]\n",
    "query2=template.format(context=relevant_texts2,question=user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=llm.invoke(query2)\n",
    "answer=response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Self-Query Retrieval (Self-RAG)** is an advanced technique used in Retrieval-Augmented Generation (RAG) models that enables the model to refine its own retrieval and generation process by iterating on its outputs.\n",
      "\n",
      "In Self-RAG, the model does not solely rely on the initial retrieval of documents or knowledge snippets but actively re-evaluates and adjusts its approach by generating follow-up queries and responses. This iterative process allows the model to:\n",
      "\n",
      "* **Correct its own mistakes:** By evaluating the accuracy and relevance of its initial response, the model can identify and correct any errors or inconsistencies.\n",
      "* **Fill in gaps:** If the initial retrieval does not provide enough information to generate a comprehensive response, the model can generate follow-up queries to gather additional relevant data.\n",
      "* **Enhance the quality of the final output:** By iteratively refining its retrieval and generation process, the model can produce more nuanced, accurate, and contextually rich responses.\n",
      "\n",
      "Self-RAG empowers RAG models to be more adaptive and precise, particularly in scenarios where a single round of retrieval might not be sufficient to provide the best possible answer. It essentially enhances the model's ability to self-correct and improve its answers, leading to higher-quality outputs.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
