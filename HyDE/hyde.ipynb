{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "api_key=os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI,GoogleGenerativeAIEmbeddings\n",
    "generation_config = {\n",
    "            \"temperature\": 0.2,\n",
    "            \"top_p\": 0.9,\n",
    "            \"max_output_tokens\": 4096,\n",
    "            \"response_mime_type\": \"application/json\"\n",
    "        }\n",
    "llm=ChatGoogleGenerativeAI(model=\"gemini-pro\",generation_config=generation_config)\n",
    "embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader  #load data\n",
    "\n",
    "load=WebBaseLoader(\"https://www.falkordb.com/blog/advanced-rag/\")\n",
    "data=load.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter  #text_splitter\n",
    "text=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "docs=text.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS   #vector_Store \n",
    "vector_stores=FAISS.from_documents(documents=docs,embedding=embedding)\n",
    "retriever=vector_stores.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":3})  #similarity search and maximum no chunk is 3\n",
    "vector_stores.save_local(\"rag_index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human:  \\nGiven the question: What is Self-Query Retrieval in rag ?, write a detailed and informative passage that provides an answer, explanation, or context about the topic. Be specific and concise, \\nfocusing on relevant facts, examples, and insights. The response should resemble an excerpt from an article, book, or scholarly discussion.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate #HyDE logic\n",
    "template=\"\"\" \n",
    "Given the question: {question}, write a detailed and informative passage that provides an answer, explanation, or context about the topic. Be specific and concise, \n",
    "focusing on relevant facts, examples, and insights. The response should resemble an excerpt from an article, book, or scholarly discussion.\n",
    "\"\"\"\n",
    "prompt_hyde=ChatPromptTemplate.from_template(template)\n",
    "user_question=\"What is Self-Query Retrieval in rag ?\"\n",
    "query=prompt_hyde.format(question=user_question)  #format provides replacing your variable into the variable in template. 'question' is variable in template 'user question' is your variable out of template . it has been replaced\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Self-Query Retrieval in RAG**\\n\\nSelf-Query Retrieval (SQR) in Retrieval-Augmented Generation (RAG) is a technique that enables a RAG model to retrieve relevant information from a knowledge base to augment its text generation process. Specifically, SQR involves generating self-queries based on the input text and then using these queries to retrieve relevant documents from the knowledge base.\\n\\nIn RAG, the self-query is generated by a self-query generator module, which takes the input text as input. The self-query generator is typically a neural network model, trained on a dataset of input texts and corresponding relevant documents. The self-query is designed to capture the main information need of the input text.\\n\\nOnce the self-query is generated, it is used to retrieve relevant documents from the knowledge base. The retrieval module in RAG is typically based on a vector-based search algorithm, such as BM25 or TF-IDF. The retrieval module ranks the documents in the knowledge base based on their relevance to the self-query.\\n\\nThe retrieved documents are then used to augment the text generation process. The text generator module in RAG integrates the information from the retrieved documents into the generated text. This can help to improve the accuracy, coherence, and informativeness of the generated text.\\n\\nSQR in RAG has shown promising results in various text generation tasks, including question answering, summarization, and dialogue generation. By enabling the model to retrieve relevant information from a knowledge base, SQR helps to enhance the quality and informativeness of the generated text.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothetical_answer=llm.invoke(query).content \n",
    "hypothetical_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.falkordb.com/blog/advanced-rag/', 'title': 'Advanced RAG Techniques: What They Are & How to Use Them', 'description': 'Master advanced RAG techniques to enhance AI performance, accuracy, and efficiency. Learn methods for optimizing retrieval and generation in complex queries.', 'language': 'en-US'}, page_content='Every RAG application can be broken down into two phases: retrieval and generation. First, RAG retrieves relevant documents or knowledge snippets from external sources, such as knowledge graphs or vector stores, using search and indexing techniques. This retrieved data is then fed into a language model, which generates contextually rich and accurate responses by synthesizing the retrieved information with its pre-trained knowledge.RAG systems have evolved as the requirements have become more'),\n",
       " Document(metadata={'source': 'https://www.falkordb.com/blog/advanced-rag/', 'title': 'Advanced RAG Techniques: What They Are & How to Use Them', 'description': 'Master advanced RAG techniques to enhance AI performance, accuracy, and efficiency. Learn methods for optimizing retrieval and generation in complex queries.', 'language': 'en-US'}, page_content='Self-RAG is an advanced technique that empowers your system to refine its own retrieval and generation process by iterating on its outputs. In Self-RAG, the model doesn’t just rely on the initial retrieval but actively re-evaluates and adjusts its approach by generating follow-up queries and responses. This iterative process allows the model to correct its own mistakes, fill in gaps, and enhance the quality of the final output.You can think of Self-RAG as your model’s ability to self-correct'),\n",
       " Document(metadata={'source': 'https://www.falkordb.com/blog/advanced-rag/', 'title': 'Advanced RAG Techniques: What They Are & How to Use Them', 'description': 'Master advanced RAG techniques to enhance AI performance, accuracy, and efficiency. Learn methods for optimizing retrieval and generation in complex queries.', 'language': 'en-US'}, page_content='Retrieval-Augmented Generation (RAG) has become a mainstream approach for working with large language models (LLMs) since its introduction in early research. At its core, RAG gathers knowledge from various sources and generates answers using a language model. However, with basic RAG, also known as Naive RAG, you may encounter challenges in obtaining accurate results for complex queries and face slow response times and higher costs when dealing with large datasets.To address these challenges,')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#According to the answer to our question answered by llm, getting the relevant chunks from the document \n",
    "relevant_docs=retriever.invoke(hypothetical_answer)\n",
    "relevant_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Every RAG application can be broken down into two phases: retrieval and generation. First, RAG retrieves relevant documents or knowledge snippets from external sources, such as knowledge graphs or vector stores, using search and indexing techniques. This retrieved data is then fed into a language model, which generates contextually rich and accurate responses by synthesizing the retrieved information with its pre-trained knowledge.RAG systems have evolved as the requirements have become more/nSelf-RAG is an advanced technique that empowers your system to refine its own retrieval and generation process by iterating on its outputs. In Self-RAG, the model doesn’t just rely on the initial retrieval but actively re-evaluates and adjusts its approach by generating follow-up queries and responses. This iterative process allows the model to correct its own mistakes, fill in gaps, and enhance the quality of the final output.You can think of Self-RAG as your model’s ability to self-correct/nRetrieval-Augmented Generation (RAG) has become a mainstream approach for working with large language models (LLMs) since its introduction in early research. At its core, RAG gathers knowledge from various sources and generates answers using a language model. However, with basic RAG, also known as Naive RAG, you may encounter challenges in obtaining accurate results for complex queries and face slow response times and higher costs when dealing with large datasets.To address these challenges,']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_texts=[\"/n\".join(doc.page_content for doc in relevant_docs)]\n",
    "relevant_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general template for rag implementations\n",
    "template=\"\"\"  \n",
    "answer the following question in detailed based on context:\n",
    "context:{context}\n",
    "question:{question}\n",
    "\"\"\"\n",
    "prompt=ChatPromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=prompt.format(context=relevant_texts,question=user_question) #^^# this response is taken based on HyDE Method \n",
    "response=llm.invoke(query)\n",
    "hyde_answer=response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Query Retrieval (Self-RAG) is an advanced technique used in Retrieval-Augmented Generation (RAG) systems. In Self-RAG, the RAG system iteratively refines its retrieval and generation process by generating follow-up queries and responses. This allows the model to self-correct, fill in gaps, and enhance the quality of the final output.\n",
      "\n",
      "Unlike basic RAG systems, which rely solely on the initial retrieval of documents or knowledge snippets, Self-RAG actively re-evaluates and adjusts its approach. This iterative process enables the model to:\n",
      "\n",
      "1. **Correct its own mistakes:** If the initial retrieval or generation step produces inaccurate or incomplete results, the model can generate follow-up queries to gather additional information and refine its response.\n",
      "\n",
      "2. **Fill in gaps:** Self-RAG allows the model to identify and address gaps in its knowledge or the retrieved documents. By generating follow-up queries, the model can gather missing information and incorporate it into its response, leading to more comprehensive and accurate outputs.\n",
      "\n",
      "3. **Enhance the quality of the final output:** Through the iterative process of self-query retrieval, the model can gradually refine its retrieval and generation strategies, resulting in higher-quality responses that are more contextually rich and accurate.\n",
      "\n",
      "Self-RAG is particularly useful in scenarios where the initial retrieval or generation steps may not be sufficient to provide complete or accurate answers. It empowers the RAG system to continuously improve its performance and deliver better results.\n"
     ]
    }
   ],
   "source": [
    "print(hyde_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this response is taken  from relevant documents based on user question\n",
    "relevant_docs2=retriever.invoke(user_question)\n",
    "relevant_texts2=[\"/n\".join(doc.page_content for doc in relevant_docs2)]\n",
    "query2=template.format(context=relevant_texts2,question=user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=llm.invoke(query2)\n",
    "answer=response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Query Retrieval in RAG (Self-RAG) is an advanced technique that allows the model to refine its own retrieval and generation process by iterating on its outputs. In Self-RAG, the model doesn't just rely on the initial retrieval but actively re-evaluates and adjusts its approach by generating follow-up queries and responses. This iterative process allows the model to correct its own mistakes, fill in gaps, and enhance the quality of the final output.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
