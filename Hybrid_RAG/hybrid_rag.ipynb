{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1276a6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders.parsers import RapidOCRBlobParser\n",
    "from langchain.retrievers import EnsembleRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cccc829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path=\"22365_3_Prompt Engineering_v7 (1).pdf\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40836ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Extracted content from page 1:\n",
      "Prompt  \n",
      "Engineering\n",
      "Author: Lee Boonstra\n",
      "\n",
      " Metadata:\n",
      "{'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 20.2 (Macintosh)', 'creationdate': '2025-03-17T13:40:21-06:00', 'moddate': '2025-03-17T13:40:26-06:00', 'trapped': '/False', 'source': '22365_3_Prompt Engineering_v7 (1).pdf', 'total_pages': 68, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "#  Initialize the loader with image extraction enabled\n",
    "loader = PyPDFLoader(\n",
    "    file_path=pdf_path,\n",
    "    extract_images=False,\n",
    "    images_parser=RapidOCRBlobParser()\n",
    ")\n",
    "\n",
    "#  Load the documents (this processes both text and image-based content)\n",
    "docs = loader.load()\n",
    "\n",
    "#  Display the first few lines and metadata of the first page\n",
    "print(\" Extracted content from page 1:\")\n",
    "print(docs[0].page_content)  # Show first 500 characters\n",
    "print(\"\\n Metadata:\")\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c99d0959",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,  # %10-20 of chunk size. It Provides strong contextual chunks based on overlapping number.\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]  # paragraph,newline,sentence,word..\n",
    ")\n",
    "\n",
    "# Split documents\n",
    "chunked_documents = text_splitter.split_documents(docs)\n",
    "documents = chunked_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04e15a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 123 documents into 123 chunks\n"
     ]
    }
   ],
   "source": [
    "print(f\"Split {len(documents)} documents into {len(chunked_documents)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b198ae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "gem_api_key=os.getenv(\"GEMINI_API_KEY\") # Get Api key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "baa5b686",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25=BM25Retriever.from_documents(documents,k=5) #Bm25 retriever(keyword search)\n",
    "vectorstore=FAISS.from_documents(documents=documents,embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",google_api_key=gem_api_key)) # semantic  retriever\n",
    "semantic_retriever=vectorstore.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":5}) \n",
    "hybrid_retriever=EnsembleRetriever(retrievers=[bm25,semantic_retriever],weights=[0.5,0.5]) #Hybrid approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c2f4682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5  documents\n",
      "Rank 1:\n",
      "Prompt Engineering\n",
      "February 2025\n",
      "18\n",
      "System, contextual and role prompting\n",
      "System, contextual and role prompting are all techniques used to guide how LLMs generate \n",
      "text, but they focus on different aspects:\n",
      "• System prompting sets the overall context and purpose for the language model. It \n",
      "defines the ‘big picture’ of what the model should be doing, like translating a language, \n",
      "classifying a review etc.\n",
      "• Contextual prompting provides specific details or background information relevant to \n",
      "the current conversation or task. It helps the model to understand the nuances of what’s \n",
      "being asked and tailor the response accordingly.\n",
      "• Role prompting assigns a specific character or identity for the language model to adopt. \n",
      "This helps the model generate responses that are consistent with the assigned role and its \n",
      "associated knowledge and behavior.\n",
      "There can be considerable overlap between system, contextual, and role prompting. E.g. a\n",
      "----------\n",
      "Rank 2:\n",
      "Introduction 6\n",
      "Prompt engineering 7\n",
      "LLM output configuration 8\n",
      "Output length 8\n",
      "Sampling controls 9\n",
      "Temperature 9\n",
      "Top-K and top-P 10\n",
      "Putting it all together 11\n",
      "Prompting techniques 13\n",
      "General prompting / zero shot 13\n",
      "One-shot & few-shot 15\n",
      "System, contextual and role prompting 18\n",
      "System prompting 19\n",
      "Role prompting 21\n",
      "Contextual prompting 23\n",
      "Table of contents\n",
      "----------\n",
      "Rank 3:\n",
      "of Modern Art (MoMA). Gaze upon masterpieces that will boggle your \n",
      "mind and make you question whether your stick-figure drawings have any \n",
      "artistic merit.\n",
      "3. Shop ‘Til You Drop on Fifth Avenue: Indulge in some retail therapy on the \n",
      "iconic Fifth Avenue. Brace yourself for sticker shock as you window-shop (or \n",
      "actually shop) at designer boutiques that will make your wallet cry. But hey, \n",
      "you’re in Manhattan, so you might as well embrace the fabulousness!\n",
      "Table 6. An example of role prompting with a humorous tone and style\n",
      "Contextual prompting\n",
      "By providing contextual prompts, you can help ensure that your AI interactions are as \n",
      "seamless and efficient as possible. The model will be able to more quickly understand your \n",
      "request and be able to generate more accurate and relevant responses, as you can see in the \n",
      "example of Table 7.\n",
      "----------\n",
      "Rank 4:\n",
      "van Gogh. The museum houses the largest collection of his paintings and \n",
      "drawings, including “The Starry Night” and “Sunflowers.”\n",
      "3. Stedelijk Museum Amsterdam: Discover modern and contemporary art \n",
      "from around the world. The museum’s collection includes works by Picasso, \n",
      "Kandinsky, and Rietveld, housed in a striking modern building.\n",
      "Table 5. An example of role prompting\n",
      "The above example shows an example of taking the role of a travel agent. When you \n",
      "would change the role to a teacher of geography, you would notice that you will receive a \n",
      "different response.\n",
      "Defining a role perspective for an AI model gives it a blueprint of the tone, style, and focused \n",
      "expertise you’re looking for to improve the quality, relevance, and effectiveness of your \n",
      "output.\n",
      "----------\n",
      "Rank 5:\n",
      "Prompt Engineering\n",
      "February 2025\n",
      "19\n",
      "Distinguishing between system, contextual, and role prompts provides a framework for \n",
      "designing prompts with clear intent, allowing for flexible combinations and making it easier to \n",
      "analyze how each prompt type influences the language model’s output.\n",
      "Let’s dive into these three different kinds of prompts.\n",
      "System prompting\n",
      "Table 3 contains a system prompt, where I specify additional information on how to return the \n",
      "output. I increased the temperature to get a higher creativity level, and I specified a higher \n",
      "token limit. However, because of my clear instruction on how to return the output the model \n",
      "didn’t return extra text.\n",
      "Goal Classify movie reviews as positive, neutral or negative.\n",
      "Model gemini-pro\n",
      "Temperature 1 Token Limit 5\n",
      "Top-K 40 Top-P 0.8\n",
      "Prompt Classify movie reviews as positive, neutral or negative. Only \n",
      "return the label in uppercase.\n",
      "Review: \"Her\" is a disturbing study revealing the direction\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "query=\"system , contextual and role prompting explain\"\n",
    "bm25_results=bm25.invoke(input=query)     # Bm25 results\n",
    "print(f\"Found {len(bm25_results) }  documents\")\n",
    "for i,doc in enumerate(bm25_results):\n",
    "    print(f\"Rank {i+1}:\")\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cc04196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Question: system , contextual and role prompting explain\n",
      "\n",
      " Answer:\n",
      "System prompting sets the overall context and purpose for the language model.  It defines the big picture of what the model should be doing (e.g., translating a language, classifying a review).\n",
      "\n",
      "Contextual prompting provides specific details or background information relevant to the current conversation or task.  It helps the model understand the nuances of what's being asked and tailor the response accordingly.\n",
      "\n",
      "Role prompting assigns a specific character or identity for the language model to adopt. This helps the model generate responses consistent with the assigned role and its associated knowledge and behavior.  There can be overlap between these three types of prompting.\n",
      "\n",
      " Source Documents:\n",
      "--- Source 1 (Page 18) ---\n",
      "Prompt Engineering\n",
      "February 2025\n",
      "18\n",
      "System, contextual and role prompting\n",
      "System, contextual and role prompting are all techniques used to guide how LLMs generate \n",
      "text, but they focus on different aspects:\n",
      "• System prompting sets the overall context and purpose for the language model. It \n",
      "defines t...\n",
      "Source: 22365_3_Prompt Engineering_v7 (1).pdf, Page: 17\n",
      "--- Source 2 (Page 3) ---\n",
      "Introduction 6\n",
      "Prompt engineering 7\n",
      "LLM output configuration 8\n",
      "Output length 8\n",
      "Sampling controls 9\n",
      "Temperature 9\n",
      "Top-K and top-P 10\n",
      "Putting it all together 11\n",
      "Prompting techniques 13\n",
      "General prompting / zero shot 13\n",
      "One-shot & few-shot 15\n",
      "System, contextual and role prompting 18\n",
      "System prompting 19\n",
      "...\n",
      "Source: 22365_3_Prompt Engineering_v7 (1).pdf, Page: 2\n",
      "--- Source 3 (Page 19) ---\n",
      "Prompt Engineering\n",
      "February 2025\n",
      "19\n",
      "Distinguishing between system, contextual, and role prompts provides a framework for \n",
      "designing prompts with clear intent, allowing for flexible combinations and making it easier to \n",
      "analyze how each prompt type influences the language model’s output.\n",
      "Let’s dive i...\n",
      "Source: 22365_3_Prompt Engineering_v7 (1).pdf, Page: 18\n",
      "--- Source 4 (Page 18) ---\n",
      "associated knowledge and behavior.\n",
      "There can be considerable overlap between system, contextual, and role prompting. E.g. a \n",
      "prompt that assigns a role to the system, can also have a context.\n",
      "However, each type of prompt serves a slightly different primary purpose:\n",
      "• System prompt: Defines the model...\n",
      "Source: 22365_3_Prompt Engineering_v7 (1).pdf, Page: 17\n",
      "--- Source 5 (Page 23) ---\n",
      "of Modern Art (MoMA). Gaze upon masterpieces that will boggle your \n",
      "mind and make you question whether your stick-figure drawings have any \n",
      "artistic merit.\n",
      "3. Shop ‘Til You Drop on Fifth Avenue: Indulge in some retail therapy on the \n",
      "iconic Fifth Avenue. Brace yourself for sticker shock as you windo...\n",
      "Source: 22365_3_Prompt Engineering_v7 (1).pdf, Page: 22\n",
      "--- Source 6 (Page 22) ---\n",
      "van Gogh. The museum houses the largest collection of his paintings and \n",
      "drawings, including “The Starry Night” and “Sunflowers.”\n",
      "3. Stedelijk Museum Amsterdam: Discover modern and contemporary art \n",
      "from around the world. The museum’s collection includes works by Picasso, \n",
      "Kandinsky, and Rietveld, h...\n",
      "Source: 22365_3_Prompt Engineering_v7 (1).pdf, Page: 21\n",
      "--- Source 7 (Page 21) ---\n",
      "as the model can craft its responses to the specific role that it has been assigned.\n",
      "For example, you could role prompt a gen AI model to be a book editor, a kindergarten \n",
      "teacher, or a motivational speaker. Once the model has been assigned a role, you can then \n",
      "give it prompts that are specific to ...\n",
      "Source: 22365_3_Prompt Engineering_v7 (1).pdf, Page: 20\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=gem_api_key) \n",
    "\n",
    "\n",
    "# This chain will take the query, use the retriever to get context,\n",
    "# and then pass the context and query to the LLM to generate the answer.\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\", # 'stuff' puts all documents into the prompt\n",
    "    retriever=hybrid_retriever,\n",
    "    return_source_documents=True #  return the docs used to answer\n",
    ")\n",
    "\n",
    "#  Run the chain with  query\n",
    "print(f\"\\n Question: {query}\")\n",
    "response = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "\n",
    "print(\"\\n Answer:\")\n",
    "print(response[\"result\"])\n",
    "\n",
    "if \"source_documents\" in response:\n",
    "    print(\"\\n Source Documents:\")\n",
    "    for i, doc in enumerate(response[\"source_documents\"]):\n",
    "        print(f\"--- Source {i+1} (Page {doc.metadata.get('page_label', 'N/A')}) ---\")\n",
    "        print(doc.page_content[:300] + \"...\") # Show beginning of the source chunk\n",
    "        print(f\"Source: {doc.metadata.get('source', 'N/A')}, Page: {doc.metadata.get('page', 'N/A')}\") # More detailed source info\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
